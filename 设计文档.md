##### 数据处理流程设计文档
        
###### 设计概要
       
          data-process-functional-service：
              主要是数据的转换操作，目前支持的是数据编码、解码、正则表达式、分割等功能、富化函数、规则引擎、
           其他数据源查询、自定义规则、子流程调用、指定某些命令在某些条件下跳过执行          
          
          设计方式：将这些一系列的数据变换操作抽象成Command对不同的Command可以进行不同方式的线性组合，
                    可以达到对数据不同处理流程的定义。通过构造不同的Command的方式向外提供可调用的API，
                    同时第三方可以按需求定义自己的Command满足业务要求。
        
          data-manage-service：这个组件主要是负责数据类型的定义（Schema的定义、字段的定义、存储策略等）和管理(数据类型的
         CURD、权限)、数据轨迹的管理（可以进行溯源）等方面的功能。定义好数据的管理程度决定着企业的数据和业务之间结合紧密
         度高低与否，所以良好的数据管理系统能促进企业的业务发展。
         设计功能规划：Schema的定义、字段的定义、存储策略等；数据类型的CURD、权限；数据轨迹的管理
           
           data-process-flow：这个组件是数据处理流程、是想描述一条数据或某个类型的数据经过哪些主要步骤的处理。
           它的设计主要是连接数据类型和数据处理作业之间的枢纽；
           它的设计要求比较高：1.需要考虑能扩展所有的不同作业方式的系统，例如：Spark、Flink、Hadoop等
                               2.目的就是通过对数据流程的定义，系统能自动生成该数据流程的作业任务。
                               最后的效果就是：用户在界面上定义好一套数据处理流程，选择Flink作业方式；
                               然后submit该Job到Flink集群上运行，用户就完成了作业的开发、运行。        
         
        关键点：
            1.是如何定义数据处理节点的模型、需要结合数据处理功能模块、作业系统的特点（Flink、Spark）等去普适这些框架
            2.数据流程需要将数据类型结合起来定义流程处理节点
            3.数据处理流程、子流程、之间的关系和作业运行怎么处理
        
        当前版本设计：只按Flink的数据处理框架去适配、上述的数据处理功能服务主要是针对数据的变换进行封装；但是实际数据处理中
        还会涉及到数据的查询、过滤、聚合（窗口计算）、流关联等操作；对这些操作的融合，可以更多的考虑FlinkSQL的方式进行集成。
        
        
        
##### 系统存在的缺点
       1.因为数据转换不可能覆盖所有的具体场景、所以支持的有限度，一些复杂的业务计算转换还是需要手动编写代码。这个还是需要
         衡量自己应用的场景去制定数据转换方式实现 
         
         
##### 实践过程难点
      1.如何划分流处理框架下的根据描述节点生成对应的算子节点，这个实现就可以很好的将数据处理流程图----翻译为-->可执行程序
        （如何能覆盖80%的场景）,难以确定的点在于我不确定所实现的方式能否覆盖大部分场景!!，理论上是都可以解决的
        
        我们需要深入理解数据处理流程，主要就是数据的变换，然后就是数据的查询、聚合查询和入库
        

##### 2019.08.23下一阶段的计划

       想写点自己的代码，有时候不知道写点什么合适，实现一些基本的需求；也是业余的一种技能积累。在这一段的工作中，完成的还是
       可以的了。那么，下一阶段需要如何安排呢？
       
        所以后续想做两个事情：
        1.将 ETL和streamDB联系起来，统一操作开发平台（废弃）
        2.将数据业务流程图进行存储、可以清楚描述出流程的信息，帮助于业务理解。
        3.动态更新配置变量这个功能如何实现有两种方式：
           1.利用发布与订阅的机制(开启web server的方式直接否掉)
           2.结合广播的机制去更新参数、变量
           这两种方式的区别是：
           第一种方案需要独立的线程去监听事件然后更新配置，需要考虑变量同步的问题
           第二中方式使用流的connect方式去更新，并且不会存在变量同步的问题
           结合实际情况，我们选择第二种方案去更新配置
       
        完成的功能：
          1.数据转换流程中配置更新功能（更新数据处理流命令、例如动态刷新缓存、业务参数调整）
          2.子流程的调用功能实现
          
        收获：
           1.加强了一些对数据处理流程的理解和设计
              
       
        
 ##### 2019.09.16 
       
       1.需要注意的问题：影响程序运行，需要将>10ms耗时的command和message记录下来，进行日志分析
       2.正则表达式是一个比较耗时的坑，所以在写正则的时候都需要事先工具分析一下  
    
   
   ---------------------------------------------------------------------------------------------------------------    
       系统范围:
       
       业务范围：
       数据治理：采集治理、数据流程治理、数据存储治理、数据分析治理、
       
       1.采集治理包括对采集器的管理、升级、配置、采集方式、
       2.数据处理流程，数据汇聚、数据解析、富化计算、衍生数据、中间数据
       
       
  ##### 2019.09.26 想法和思路更新
       系统业务更新：    
          
       以数据处理流程为中心去建立整个数据计算系统，用户可以通过定义一条数据处理流程，就可以直接启动该数据
       业务处理的任务。
      
      1.建立数据流程任务的配置和任务启动之间的联系,运行任务可以从第三方配置中心加载配置。其中当前版本实现的方式
        是在运行Job之前将配置生成配置文件打入需要启动的jar包中，后续对Flink改造熟悉了，可以在其运行时获取配置
        （--这个简单的使用Http方式获取配置了）
      2.任务级别或者ETL计算级别的事件监听的实现（--这个不想做了，可以实现使用广播）
       
       技术架构上的更新：
       1.支持异步ETL计算数据转换的算子
       2.每一个ETL计算的命令可以定义输入输出字段
       
  #### 2019.10.22流程图的设计
       最近在设计流程图的问题上遇到一些问题，初始的想法是想将流程图设计的比较完美，完全从流程图效果的表述上去做，自由度高。
       但是发现会产生一些和设计冲突的问题。后来经过思考流程图分支和子流程的区别，之前一直想将分支流程看成是子流程去做，难度
       较大。后来思考到一种方式，我程序本身的设计就是支持分支流程图的算子。但是所谓子流程的定义应该是某一个节点内的子流程，
       和分支流程是有区别的。这样去设计，感觉程序本身功能更清晰明了和简单，而且支持的场景也足够多。 
       
       1.所以改为针对节点创建子流程，因为根据具体的条件在这个节点选择对应的子流程，执行完之后可以回到相应的主流程上。   
       
       在这段时间完成了流程的配置
       
       1.明天完成命令实体的创建
       2.打通从页面流程构建到生成ETL任务的通道
       
  #### 201.9.10.29 总结
        从这个项目8月出到现在也有三个月了，用工作之余的事件去构建。从开始的一些小的想法开始去设计和开发这么个东西，到现在
        感觉也有些许收获，主要体现在以下方面：
        1.项目的设计思想、应用性很重要!!!因为易用性从而产生一些设计思想
        2.必须要有清晰的设计思想（基于对项目业务理解的深度），才能使得系统关系、架构趋于完善，这一点在设计实体模型的时候
          深有体会 。
        3.对web系统设计的一些加强和理解，比如自定义验证器、DAO层、统一异常处理等。
        4.在使用JPA持久层带来的一些问题的思考，JPA用好还是不容易。而且一直困扰的一个问题就是VO如何设计是比较好的选择，
          没有想出更好的解决方式
  
  #### 2019.10.31  
       
        这个项目有了基本的功能流程了，能基本运行起来。有很多细节的功能比如修改、删除、页面等操作呀。因为核心的功能已经基
        本实现，所以其他的一些功能不打算花精力去研究了。所以整理一下下一阶段的工作：        
        1.流处理的统一应用平台，能轻量级的包装应用。
        2.在运行平台上研究一些其他的流功能特性
        3.数据特征处理的研究、把py引进来
       
        
        其他计划：         
        1.将Flow挂在具体的Job下面，对于Flow里面没有设计流计算的数据源和数据输出节点（可以考虑将其设计进去，形成完成的流程）
        2.将具体的流程挂在指定的Job下面，通过将Job配置进行下发到大数据平台服务（需要开发），这样平台服务会拉取指定的jar包
          和任务配置一并提交的Flink的任务中去，这样就可以完成任务的配置、下发、启动等流程。
          
         流程概念：
         1.主流程的实现         
         2.条件流程，在做条件流程的时候总是和子流程混淆了。
            该场景下的设计：
            子流程是某个节点下面有一个或多个子流程，执行完之后，会回到主流程上来。
            条件分支流程在该场景下也可以看成是某个node下选择的流程，但是在设计的时候将分支流程做一个特定的Node去处理。
            
   ##### 2019.11.14
             
            1.关于流程配置更新，在流程级别更新的时候，一般都需要重新build command chain，然后替换上去。
            2.对于动态增加command jar包的时候，则也需要重新build commannd chain，注意需要配置好jar包的路径。
            
            
   ##### 2019.11.19   
       
     
         关于字段元数据管理这一块可能需要重构一下，目前的这个关系模型是将所有的字段都放在一张表中，用标识符
         字段进行分类，这样不是特别好。所以后续可以考虑按不同类型功能拆成对应的表
         
         功能性补充其他需求：
         1.关于任务的可配置参数，CP参数、从CP或SP启动等
         2.关于kafka启动配置如消费模型等等配置的完善
         3.Jar包的管理(注意还是要使用Flink的Job的隔离机制进行实现，在Flink环境下不能使用MorphlineContext加载包)-->
            通过代码页面化实现（可以通过页面直接实现类的编译部署）
         4.关于流程调试功能可以支持节点调试和全流程调试或者指定流程中部分节点的调试
         5.任务状态监控操作，需要判断任务假死得状态，重启机制指定 
         
         血缘关系的设计：
         1、对表、字段要有统一的管理、相当于时一个注册中心、所有的表的创建都是走统一的入口。这里其实需要设计统一的
            API去规范化或者是改造旧的系统的时候需要对表进行再注册。对于表的修改或者删除，只添加纪录进行标记！！
         2、分析表、可以分析出热点表、热点字段、是原始字段、还是中间结果表或者字段等，更高层的业务需要挖掘
         
         
   ##### 2019.11.27
        
        从项目的实际对业务的作用出发去设计系统，如何对业务
          
        实时数仓计算平台可以划分为两个部分去设计：
        1.ETL平台专注于数据从数据源接入、业务逻辑处理和按业务需求决定是否sink
        2.一般如果经过ETL处理之后的数据需要进行实时查询计算的数据，所以这里有一个问题需要考虑
          后面的查询计算时从ETL处理流直接计算还是，将ETL的数据sink到kafka。
          方案2可能好一点：将数据操作解耦，但是会有资源的占有，kafka资源会翻倍，数据处理的实时性稍差，
          因为要等ETL将数据写入到Kafka后，再消费。          
        3.事件数据的监控平台、也就是基于查询或者CEP查询满足的数据 这个都可以归到步骤2中去
        
        
   ##### 2019.11.29
            围绕目标和度量去思考问题，做任何一件事我们必须基于背景分析我们所做的事情的实际价值，并将其度量出来。
           这样才可能做出一个有意义的产品。目标很重要，我们在设计一个大数据中台的时候需要考虑数据中台带来的价值
           ，中台的作用应该具有共享、通过数据和业务的共享能对数据和业务进行分析反馈、形成中台链路(综合治理)等三个
           作用，我认为是比较有价值的！ 
           
            大数据中台设计围绕数据仓库进行设计、良好的数据仓库设计是连接数据处理和业务使用查询的桥梁。
            
            数据仓库的设计：
            
            1.元数据中心：存储管理、表管理、字段管理
              因为元数据代表了可用的数据资产、元数据能描述这份数据的生命周期信息。              
              如何建立元数据模型？              
              
            2.数据资产：围绕元数据中心模型的实现我们可以从中获取数据资产信息  
              基于数据血缘，数据的访问热度分析，做成本的治理
              重点是需要针对数据血缘的业务进行梳理，建立良好的功能模型。
              
            3.数据质量----数据价值的体现
              上述两个步骤是基本是对元数据、数据和业务之间的关系的一种描述，还不能产生实际应用价值（从前面两点的价值
               只是整理系统内数据贵范和易用性，没有商业价值，但是它是所有其他的系统、业务的基础设施）
               那么究竟该如何去挖掘这些数据的价值呢？
               这个价值与背后的业务是息息相关的，只有针对特定的产品和业务才能制定相应的数据分析--->营销决策---->盈利
               
               先不谈具体的实现、可以从需要具有的功能谈起：
               1.从基础数据设施，探索各数据、业务部门之间的关系，纵横多维度去分析。
               问题1:分析啥？目标是什么
                    既然提到分析那就需要从结果出发，你能从结果得到什么，如何促进系统、业务、商业的正向变化。
                   所以这个需求在这里的分析是不合理的。需要理解实际的业务背景，才能知道如何去促进商业的发展
                   
            4.越过3我们需要做的就是数据基础设施的建设，如何把底层的模型实现，是目前可以考虑的。对原始数据的管理，可以
              知道不同业务方为什么需要这些数据、这些数据拿去如何建模的、如何分析的都是可以知道的。这一部分也是很重要的
              一些应用中产生的数据。
              
             综上述，我们需要建设完善的元数据管理和数据的血缘轨迹、以及各业务平台对数据的使用情况（将业务之间的联系）
             。这样有了底层的系统模型的建设，至于上层的商业模型可以视具体情况而定。
             理清了思路之后，可以先做各业务共性的基础数据中台，这样上层建设是可以复用基础“中台”。从这个角度来说，所有
             的事情有了共性、共享可行那么都可称之为“某中台”，在这里可以称之为大数据基础技术中台。
             我们还是不要把中台的定义定的太细，例如搜索中台什么的，没啥意义，这样向下划分。只强调从宏观上划分成业务中台
             和基础技术中台两个部分。那我的下一阶段的目标就出现了，想尝试去实现一个大数据基础技术中台，为业务提供一个
             良好的技术和数据支撑的平台。
             
             
             结合上述背景分析可行性的需求实现：
             1.可以基于Neo4j的技术去实现这些数据字段、表、APP、业务等之间的关系
             
             2.数据表和APP之间的关系
                可以围绕APP应用去关联这些字段、表等信息，因为字段的生成、处理、入库、使用等等生命周期是由某一系列的APP去
              使用的，那么只要理清这些具体的应用处理关系就可以知道这个数据被使用的脉络关系。
              
             3.字段和和APP之间的关系              
                  其实就是可以统计出表、字段的热点、能形成数据地图，了解表和字段的生命周期、以及被那些业务重点使用关系。
                  
             4.在分析出重要的表和字段
                  这个就可以推给具体的业务去做业务建模分析、形成自己的业务反馈
             
              
                       
              
              
              
              
              
              
              
              
               
                
                
                
                
                
                
                
                
                
           
                
             
             
             
             
             
             
             
               
                
                   
                    
                    
               
                 
          
          
        
          
          
       
           
        
          
        
   
               
                   
                  
            
            
          
          
      
                      
        
        
        
            
          
           
           
          
          
        
        
        
        
                